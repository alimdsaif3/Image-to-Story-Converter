ğŸ¨ Image to Speech GenAI Tool Using LLM ğŸ¤ğŸ“·

An AI-powered tool that transforms uploaded images into audio short stories using Hugging Face models, OpenAI, and LangChain. Deployed on Streamlit Cloud and Hugging Face Spaces for easy access! ğŸš€



ğŸ“¢ Run the App

ğŸ”— Launch on StreamlitğŸ”— Launch on HuggingFace Space

ğŸ¯ Demo

Here are some examples of how the tool works:


You can listen to the respective audio file of this test demo image in the img-audio folder.


You can listen to the respective audio file of this test image in the img-audio folder.


You can listen to the respective audio file of this test image in the img-audio folder.

ğŸ“ˆ System Design



ğŸ† Approach

An app that uses Hugging Face AI models to generate text from an image, which then generates audio from the text.

Execution is divided into 3 parts:

Image to Text:An image-to-text transformer model (Salesforce/blip-image-captioning-base) is used to generate a text scenario based on the AI understanding of the image context.

Text to Story:OpenAI LLM model is prompted to create a short story (50 words: can be adjusted as required) based on the generated scenario. (gpt-3.5-turbo)

Story to Speech:A text-to-speech transformer model (espnet/kan-bayashi_ljspeech_vits) is used to convert the generated short story into a voice-narrated audio file.

A user interface is built using Streamlit to enable uploading the image and playing the audio file.

ğŸŒŸ Requirements

os

python-dotenv

transformers

torch

langchain

openai

requests

streamlit

ğŸš€ Usage

Before using the app, the user should have personal tokens for Hugging Face and OpenAI.

The user should set up a virtual environment (venv) and install the ipykernel library for running the app on a local system IDE.

Save the personal tokens in a .env file within the package as string objects under the object names: HUGGINGFACE_TOKEN and OPENAI_TOKEN.

Run the app using the command: streamlit run app.py.

Once the app is running on Streamlit, the user can upload the target image.

Execution will start automatically, and it may take a few minutes to complete.

Once completed, the app will display:

The scenario text generated by the image-to-text transformer Hugging Face model.

The short story generated by prompting the OpenAI LLM.

The audio file narrating the short story generated by the text-to-speech transformer model.

The Gen AI App is deployed on Streamlit Cloud and Hugging Face Space.

â–¶ï¸ Installation

Clone the repository:

git clone https://github.com/alimdsaif3/Image-to-Story-Converter.git

Install the required Python packages:

pip install -r requirements.txt

Set up your OpenAI API key & Hugging Face Token by creating a .env file in the root directory of the project with the following contents:

OPENAI_API_KEY=<your-api-key-here>
HUGGINGFACE_API_TOKEN=<your-access-token-here>

Run the Streamlit app:

streamlit run app.py

Â©ï¸ License

Distributed under the MIT License. See LICENSE for more information.

If you like this LLM Project, do drop a â­ to this repo! Contributions are welcome! If you have any suggestions for improving this AI Image-to-Speech Converter, please submit a pull request. ğŸ™‡

Follow me on:

