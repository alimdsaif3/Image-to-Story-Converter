🎨 Image to Speech GenAI Tool Using LLM 🎤📷

An AI-powered tool that transforms uploaded images into audio short stories using Hugging Face models, OpenAI, and LangChain. Deployed on Streamlit Cloud and Hugging Face Spaces for easy access! 🚀



📢 Run the App

🔗 Launch on Streamlit🔗 Launch on HuggingFace Space

🎯 Demo

Here are some examples of how the tool works:


You can listen to the respective audio file of this test demo image in the img-audio folder.


You can listen to the respective audio file of this test image in the img-audio folder.


You can listen to the respective audio file of this test image in the img-audio folder.

📈 System Design



🏆 Approach

An app that uses Hugging Face AI models to generate text from an image, which then generates audio from the text.

Execution is divided into 3 parts:

Image to Text:An image-to-text transformer model (Salesforce/blip-image-captioning-base) is used to generate a text scenario based on the AI understanding of the image context.

Text to Story:OpenAI LLM model is prompted to create a short story (50 words: can be adjusted as required) based on the generated scenario. (gpt-3.5-turbo)

Story to Speech:A text-to-speech transformer model (espnet/kan-bayashi_ljspeech_vits) is used to convert the generated short story into a voice-narrated audio file.

A user interface is built using Streamlit to enable uploading the image and playing the audio file.

🌟 Requirements

os

python-dotenv

transformers

torch

langchain

openai

requests

streamlit

🚀 Usage

Before using the app, the user should have personal tokens for Hugging Face and OpenAI.

The user should set up a virtual environment (venv) and install the ipykernel library for running the app on a local system IDE.

Save the personal tokens in a .env file within the package as string objects under the object names: HUGGINGFACE_TOKEN and OPENAI_TOKEN.

Run the app using the command: streamlit run app.py.

Once the app is running on Streamlit, the user can upload the target image.

Execution will start automatically, and it may take a few minutes to complete.

Once completed, the app will display:

The scenario text generated by the image-to-text transformer Hugging Face model.

The short story generated by prompting the OpenAI LLM.

The audio file narrating the short story generated by the text-to-speech transformer model.

The Gen AI App is deployed on Streamlit Cloud and Hugging Face Space.

▶️ Installation

Clone the repository:

git clone https://github.com/alimdsaif3/Image-to-Story-Converter.git

Install the required Python packages:

pip install -r requirements.txt

Set up your OpenAI API key & Hugging Face Token by creating a .env file in the root directory of the project with the following contents:

OPENAI_API_KEY=<your-api-key-here>
HUGGINGFACE_API_TOKEN=<your-access-token-here>

Run the Streamlit app:

streamlit run app.py

©️ License

Distributed under the MIT License. See LICENSE for more information.

If you like this LLM Project, do drop a ⭐ to this repo! Contributions are welcome! If you have any suggestions for improving this AI Image-to-Speech Converter, please submit a pull request. 🙇

Follow me on:

